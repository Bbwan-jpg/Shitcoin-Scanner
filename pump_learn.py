import os                       # Utilitaires syst√®me (fichiers, variables d'env)
import sys                      # (non utilis√© ici) ‚Äì pourrait servir pour argv / path
import time                     # Horodatage et temporisations
import math                     # (non utilis√© ici) ‚Äì fonctions math si besoin
import joblib                   # Sauvegarde/chargement du mod√®le sklearn
import ccxt                     # Client d‚Äôexchanges crypto (Binance ici)
import numpy as np              # Calcul num√©rique
import pandas as pd             # Manipulation de tableaux/Series
from dateutil import parser as du              # Parsing de dates ISO ‚Üí datetime
from typing import Dict, Any, List, Optional  # Types d‚Äôannotation
from sklearn.pipeline import Pipeline         # Pipeline sklearn (scaler + mod√®le)
from sklearn.preprocessing import StandardScaler   # Normalisation des features
from sklearn.linear_model import LogisticRegression # Classifieur
from sklearn.metrics import classification_report   # Rapport de performance

# ==============================
# R√©glages
# ==============================
DEFAULT_EVENTS_CSV = "pumpdump_labels_sample.csv"   # CSV des √©v√©nements (train)
DEFAULT_MODEL_PATH = "model_pump.pkl"               # Chemin du mod√®le sauvegard√©
DEFAULT_SYMBOLS_TXT = "symbols.txt"                 # Liste de tickers pour la pr√©diction

TIMEFRAME = "1m"            # Granularit√© des bougies pour features
LOOKBACK_MIN = 120          # Fen√™tre d‚Äôhistorique en minutes
PAUSE_S = 0.25              # Pause entre appels ccxt (rate limit)

# N√©gatifs auto si une seule classe
NEG_PER_POS = 2             # Nombre d‚Äô√©chantillons n√©gatifs cr√©√©s par positif
RANDOM_SEED = 42            # Graine RNG pour reproductibilit√©

# ==============================
# Helpers g√©n√©raux
# ==============================
def ok(s: str) -> bool:
    return s is not None and str(s).strip() != ""   # True si cha√Æne non vide

def to_utc_ms(s: Optional[str]) -> Optional[int]:
    if not ok(s):                                   # Ignore si vide
        return None
    try:
        return int(du.isoparse(s).timestamp() * 1000)  # ISO ‚Üí epoch ms
    except Exception:
        return None                                  # None si parsing impossible

def choose_market_symbol(exchange: ccxt.Exchange, base: str) -> Optional[str]:
    """
    Essaie base/USDT puis base/BUSD. Retourne None si introuvable.
    """
    base = base.upper().strip()                     # Normalise le ticker
    candidates = [f"{base}/USDT", f"{base}/BUSD"]   # Priorit√© aux march√©s usuels
    markets = exchange.load_markets()               # Charge la liste des march√©s
    for m in candidates:
        if m in markets:                            # Retourne le premier existant
            return m
    return None                                     # Aucun march√© trouv√©

def fetch_ohlcv_safe(exchange: ccxt.Exchange, market: str, timeframe: str, since_ms: int, limit: int = 2000):
    try:
        return exchange.fetch_ohlcv(market, timeframe=timeframe, since=since_ms, limit=limit)  # T√©l√©charge OHLCV
    except Exception:
        return []                                    # En cas d‚Äôerreur ‚Üí liste vide

def ohlcv_to_df(ohlcv: List[List[Any]]) -> pd.DataFrame:
    if not ohlcv:
        return pd.DataFrame(columns=["t","o","h","l","c","v"])  # DF vide standardis√©
    df = pd.DataFrame(ohlcv, columns=["t","o","h","l","c","v"]) # Colonnes ccxt classiques
    df["t"] = pd.to_datetime(df["t"], unit="ms", utc=True)      # ms epoch ‚Üí timestamp UTC
    return df

# ==============================
# Features √† partir des OHLCV
# ==============================
def pct_change(series: pd.Series, n: int) -> float:
    if len(series) < n+1:                           # Pas assez de points
        return 0.0
    try:
        a = float(series.iloc[-n-1])                # Valeur n p√©riodes avant
        b = float(series.iloc[-1])                  # Derni√®re valeur
        return 0.0 if a == 0 else 100.0 * (b - a) / a  # Variation % sur n
    except Exception:
        return 0.0

def rolling_vol(series: pd.Series, n: int) -> float:
    if len(series) < n:
        return 0.0
    return float(series.iloc[-n:].pct_change().std() or 0.0)  # √âcart-type des retours sur n

def volume_surge(vol: pd.Series, n_now: int = 1, n_ref: int = 60) -> float:
    """
    Ratio volume(dernier n_now) / m√©diane(volume des n_ref pr√©c√©dents).
    """
    if len(vol) < max(n_now, n_ref)+1:
        return 0.0
    now = float(vol.iloc[-n_now:].sum())            # Volume r√©cent agr√©g√©
    ref = float(vol.iloc[-(n_ref+1):-1].median() or 0.0)  # R√©f√©rence historique (m√©diane)
    return now / ref if ref > 0 else 0.0

def wick_ratio(df: pd.DataFrame) -> float:
    """
    Longueur de m√®che (H-L) vs corps (|C-O|) de la derni√®re bougie.
    """
    if df.empty:
        return 0.0
    o,h,l,c = map(float, df.iloc[-1][["o","h","l","c"]].values)  # Derni√®re bougie
    body = abs(c-o)                              # Taille du corps
    range_ = max(1e-12, h-l)                     # Amplitude totale (√©vite /0)
    return (range_ / max(1e-12, body)) if body > 0 else (range_ / 1e-6)  # Ratio m√®che/corps

def build_features_from_df(df: pd.DataFrame) -> Dict[str, float]:
    if df.empty:
        # vecteur nul
        return {k:0.0 for k in FEATURE_ORDER}    # Retourne toutes features √† 0

    close = df["c"].astype(float)                # S√©rie des cl√¥tures
    vol   = df["v"].astype(float)                # S√©rie des volumes

    feats = {
        # momentum
        "ret_5m": pct_change(close, 5),          # Variation 5 min
        "ret_15m": pct_change(close, 15),        # Variation 15 min
        "ret_60m": pct_change(close, 60),        # Variation 60 min
        # volat / microstructure
        "vola_15": rolling_vol(close, 15),       # Volatilit√© glissante 15
        "vola_60": rolling_vol(close, 60),       # Volatilit√© glissante 60
        "wick_ratio": wick_ratio(df),            # Ratio m√®ches / corps
        # volumes
        "vol_surge_1_60": volume_surge(vol, 1, 60),    # Spike vol 1 vs 60
        "vol_surge_5_60": volume_surge(vol, 5, 60),    # Spike vol 5 vs 60
        "vol_surge_15_60": volume_surge(vol, 15, 60),  # Spike vol 15 vs 60
        # niveaux
        "price": float(close.iloc[-1]),          # Dernier prix
        "log_price": float(np.log1p(max(float(close.iloc[-1]), 0.0))),  # log1p(price)
    }
    return feats

# Ordre fixe
FEATURE_ORDER = [
    "ret_5m","ret_15m","ret_60m",
    "vola_15","vola_60","wick_ratio",
    "vol_surge_1_60","vol_surge_5_60","vol_surge_15_60",
    "price","log_price"
]

def features_for_symbol_before(exchange: ccxt.Exchange, base: str, t0_ms: int) -> Optional[Dict[str,float]]:
    """
    Construit les features sur [t0 - LOOKBACK_MIN, t0] (inclus) pour SYMBOL/USDT|BUSD.
    """
    market = choose_market_symbol(exchange, base)   # Choix du march√©
    if not market:
        return None
    since = t0_ms - LOOKBACK_MIN * 60 * 1000        # D√©but de fen√™tre d‚ÄôOHLCV
    raw = fetch_ohlcv_safe(exchange, market, TIMEFRAME, since)  # T√©l√©charge les bougies
    df = ohlcv_to_df(raw)                           # Convertit en DataFrame
    # ne garder que <= t0
    df = df[df["t"].astype(np.int64)//10**6 <= t0_ms]  # Filtre les bougies futures
    return build_features_from_df(df)               # Calcule le vecteur de features

def features_for_symbol_now(exchange: ccxt.Exchange, base: str) -> Optional[Dict[str,float]]:
    """
    Construit les features sur [now - LOOKBACK_MIN, now] pour SYMBOL/USDT|BUSD.
    """
    market = choose_market_symbol(exchange, base)   # Choix du march√©
    if not market:
        return None
    now_ms = int(time.time() * 1000)                # Timestamp actuel (ms)
    since = now_ms - LOOKBACK_MIN * 60 * 1000       # D√©but de fen√™tre
    raw = fetch_ohlcv_safe(exchange, market, TIMEFRAME, since)  # R√©cup OHLCV
    df = ohlcv_to_df(raw)                           # DataFrame OHLCV
    return build_features_from_df(df)               # Vecteur de features

# ==============================
# Dataset d'entra√Ænement (CSV PumpDump)
# ==============================
def load_events_csv(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)                          # Charge le CSV des √©v√©nements
    # colonnes utilis√©es :
    # announced_at_utc, pump_date_utc (optionnel), symbol, exchange, label_numeric, is_success
    for col in ["symbol","exchange","announced_at_utc"]:
        if col not in df.columns:                   # V√©rifie les colonnes cl√©s
            raise ValueError(f"Colonne manquante : {col}")
    # normaliser
    df["symbol"] = df["symbol"].astype(str).str.upper().str.strip()   # Tickers propres
    df["exchange"] = df["exchange"].astype(str).str.strip()           # Nom d‚Äôexchange propre
    # harmoniser champs de label si pr√©sents
    if "label_numeric" in df.columns:
        df["label_numeric"] = pd.to_numeric(df["label_numeric"], errors="coerce").fillna(1).astype(int)  # Nettoie label_numeric
    if "is_success" in df.columns:
        df["is_success"] = pd.to_numeric(df["is_success"], errors="coerce").fillna(0).astype(int)        # Nettoie is_success
    return df

def pick_target_column(ev: pd.DataFrame) -> Optional[str]:
    """
    Choisit la colonne de label :
    1) is_success si binaire et contient 0 & 1
    2) label_numeric si binaire et contient 0 & 1
    3) sinon None (on g√©n√©rera des n√©gatifs synth√©tiques)
    """
    if "is_success" in ev.columns:
        uniq = ev["is_success"].dropna().astype(int).unique() # Valeurs uniques
        if set(uniq) >= {0,1}:                                 # Contient 0 et 1
            return "is_success"
    if "label_numeric" in ev.columns:
        uniq = ev["label_numeric"].dropna().astype(int).unique()
        if set(uniq) >= {0,1}:
            return "label_numeric"
    return None

def get_binance_universe(exchange: ccxt.Exchange) -> List[str]:
    """
    Renvoie la liste des bases (tickers) disponibles sur /USDT ou /BUSD.
    """
    mkts = exchange.load_markets()                 # Charge march√©s Binance
    bases = set()
    for sym in mkts.keys():                        # It√®re tous les symboles
        if sym.endswith("/USDT") or sym.endswith("/BUSD"):
            base = sym.split("/")[0]               # R√©cup√®re la base
            # on filtre quelques "bases" bizarres
            if len(base) >= 2 and base.isalpha():  # Garde des tickers plausibles
                bases.add(base.upper())
    return sorted(list(bases))                     # Liste tri√©e

def build_training_frame(events_csv: str) -> pd.DataFrame:
    print(f"Chargement des √©v√©nements : {events_csv}")  # Log
    ev = load_events_csv(events_csv)                # Lit et nettoie le CSV
    ev = ev[ev["exchange"].str.lower()=="binance"].copy()  # Garde Binance uniquement

    # t0 = pump_date_utc si dispo, sinon announced_at_utc
    ev["t0_ms"] = ev["pump_date_utc"].apply(to_utc_ms) if "pump_date_utc" in ev.columns else None  # t0 depuis pump_date_utc
    if "t0_ms" in ev:
        ev["t0_ms"] = ev["t0_ms"].fillna(ev["announced_at_utc"].apply(to_utc_ms))  # fallback announced_at_utc
    else:
        ev["t0_ms"] = ev["announced_at_utc"].apply(to_utc_ms)                      # si pas de colonne pr√©c√©dente

    ev = ev[ev["t0_ms"].notna()].copy()           # Supprime lignes sans t0

    # label de r√©f√©rence
    target_col = pick_target_column(ev)           # Choisit la colonne label
    if target_col:
        print(f"Label utilis√© = '{target_col}'")  # Info si label dispo
    else:
        print("Aucun label binaire utilisable trouv√© -> g√©n√©ration de n√©gatifs synth√©tiques‚Ä¶")

    ex = ccxt.binance({"enableRateLimit": True})  # Instancie client ccxt Binance
    rng = np.random.default_rng(RANDOM_SEED)      # RNG pour choix al√©atoires

    rows = []                                     # Accumule les vecteurs de features
    labels = []                                   # Accumule les labels
    meta = []                                     # M√©tadonn√©es (symbol, t0, kind)

    # ---- Positifs (ou lignes √©v√©nementielles) ----
    print(f"Construction des features avant t0 (lookback={LOOKBACK_MIN} min) ‚Ä¶")
    for _, r in ev.iterrows():                    # Parcourt chaque √©v√©nement
        base = r["symbol"]                         # Ticker
        t0   = int(r["t0_ms"])                     # Timestamp de r√©f√©rence
        feats = features_for_symbol_before(ex, base, t0)  # Features avant t0
        time.sleep(PAUSE_S)                        # Respecte rate limit
        if feats is None:
            continue                               # Ignore si march√© absent
        rows.append([feats.get(k,0.0) for k in FEATURE_ORDER])  # Ajoute features dans l‚Äôordre
        yval = int(r[target_col]) if target_col else 1          # Label positif si pas de colonne
        labels.append(yval)                         # Append label
        meta.append({"symbol": base, "t0": r.get("pump_date_utc") or r.get("announced_at_utc"), "kind":"pos"})  # Meta

    if not rows:
        raise RuntimeError("Aucun feature construit pour les √©v√©nements. V√©rifie les symboles/markets.")  # S√©curit√©

    # ---- Si une seule classe -> cr√©er des n√©gatifs ----
    unique_classes = set(labels)                   # Classes pr√©sentes
    if len(unique_classes) < 2:
        # construire un univers, puis √©chantillonner pour chaque positif
        uni = get_binance_universe(ex)             # Univers Binance (bases)
        pos_syms = set([m["symbol"] for m in meta])# Symboles d√©j√† utilis√©s
        pool = [s for s in uni if s not in pos_syms]  # Pool pour n√©gatifs
        if not pool:
            raise RuntimeError("Impossible de cr√©er des n√©gatifs: pool vide.")
        print(f"G√©n√©ration de n√©gatifs : {NEG_PER_POS} par positif (pool ~{len(pool)} bases)‚Ä¶")

        extra_rows, extra_y, extra_meta = [], [], []
        for m in meta:                              # Pour chaque positif
            t0 = int(to_utc_ms(m["t0"]))           # M√™me t0
            cand = rng.choice(pool, size=min(NEG_PER_POS, len(pool)), replace=False)  # √âchantillonne des bases
            for base in cand:
                feats = features_for_symbol_before(ex, base, t0)  # Features pour une base ‚Äún√©gative‚Äù
                time.sleep(PAUSE_S)
                if feats is None:
                    continue
                extra_rows.append([feats.get(k,0.0) for k in FEATURE_ORDER])  # Ajoute features
                extra_y.append(0)                                             # Label n√©gatif
                extra_meta.append({"symbol": base, "t0": m["t0"], "kind":"neg"})  # Meta n√©gatif

        print(f"N√©gatifs g√©n√©r√©s: {len(extra_rows)}")  # Log
        rows.extend(extra_rows)                        # Concat features
        labels.extend(extra_y)                         # Concat labels
        meta.extend(extra_meta)                        # Concat meta

    # ---- Construire la table finale ----
    X = pd.DataFrame(rows, columns=FEATURE_ORDER)     # Matrice X
    y = pd.Series(labels, name="label")               # S√©rie y
    M = pd.DataFrame(meta)                            # DF meta
    df = pd.concat([M, X, y], axis=1)                 # Fusion finale

    print(f"Dataset : {df.shape[0]} lignes, {len(FEATURE_ORDER)} features. R√©partition classes:")
    print(df["label"].value_counts().to_string())     # Distribution de y
    return df

# ==============================
# Mod√®le
# ==============================
def make_pipeline() -> Pipeline:
    return Pipeline([
        ("scaler", StandardScaler(with_mean=True, with_std=True)),            # Normalise les features
        ("clf", LogisticRegression(max_iter=400, class_weight="balanced"))    # R√©gression logistique √©quilibr√©e
    ])

def train_and_save(events_csv: str, model_out: str = DEFAULT_MODEL_PATH):
    df = build_training_frame(events_csv)             # Construit le dataset
    if df["label"].nunique() < 2:                     # V√©rifie pluralit√© des classes
        print("\n‚ùå Toujours une seule classe apr√®s tentative de g√©n√©ration de n√©gatifs.")
        print("   -> V√©rifie ton CSV ou augmente NEG_PER_POS.")
        return

    X = df[FEATURE_ORDER].fillna(0.0).values          # Matrice de features (NA‚Üí0)
    y = df["label"].values                            # Labels

    pipe = make_pipeline()                             # Cr√©e le pipeline
    pipe.fit(X, y)                                     # Entra√Æne le mod√®le

    # rapport ‚Äúapparent‚Äù (sur train)
    preds = pipe.predict(X)                            # Pr√©dictions sur train
    print("\n=== Rapport sur le set d'entra√Ænement (indicatif) ===")
    print(classification_report(y, preds, digits=3))   # Rapport de perf

    joblib.dump({"model": pipe, "features": FEATURE_ORDER}, model_out)  # Sauvegarde pack
    print(f"\n‚úÖ Mod√®le sauvegard√© -> {model_out}")

def explain_model(model_path: str = DEFAULT_MODEL_PATH, top: int = 15):
    pack = joblib.load(model_path)                     # Charge le pack
    pipe: Pipeline = pack["model"]                     # R√©cup pipeline
    feats: List[str] = pack["features"]                # Liste de features
    clf: LogisticRegression = pipe.named_steps["clf"]  # Mod√®le logistique
    coefs = clf.coef_.ravel()                          # Coefficients
    idx = np.argsort(np.abs(coefs))[::-1]              # Tri par |coef| d√©croissant
    print("\nFeatures les plus influentes (|coef| d√©croissant):")
    for k in idx[:top]:
        print(f"{feats[k]:<18s}  coef={coefs[k]:+ .5f}")  # Affiche top-k

# ==============================
# Pr√©diction ‚Äúmaintenant‚Äù
# ==============================
def predict_now(model_path: str = DEFAULT_MODEL_PATH,
                symbols: Optional[List[str]] = None,
                out_csv: str = "pred_now.csv",
                min_prob: float = 0.5):
    pack = joblib.load(model_path)                     # Charge le pack
    pipe: Pipeline = pack["model"]                     # R√©cup pipeline
    feats: List[str] = pack["features"]                # Liste de features

    if symbols is None:
        if os.path.exists(DEFAULT_SYMBOLS_TXT):        # Lit symbols.txt si dispo
            with open(DEFAULT_SYMBOLS_TXT, "r", encoding="utf-8") as f:
                symbols = [ln.strip().upper() for ln in f if ln.strip()]
        else:
            symbols = ["AVAX","ARPA","NEBL","BRD","PIVX","ALGO","CHZ","FXS","EZ","NAS"]  # Liste par d√©faut

    ex = ccxt.binance({"enableRateLimit": True})       # Client Binance
    rows = []                                          # Vecteurs de features
    meta = []                                          # Meta (symbol)

    print(f"Pr√©diction live sur : {', '.join(symbols)}")
    for base in symbols:                               # Parcourt les tickers
        feats_now = features_for_symbol_now(ex, base)  # Features actuelles
        time.sleep(PAUSE_S)                            # Respecte rate limit
        if feats_now is None:
            print(f"  - {base:<8s} march√© introuvable (USDT/BUSD). Ignor√©.")  # Log march√© absent
            continue
        rows.append([feats_now.get(k,0.0) for k in FEATURE_ORDER])  # Ajoute features
        meta.append({"symbol": base})                  # Ajoute meta

    if not rows:
        print("Aucun symbole exploitable.")            # Rien √† scorer
        return

    X = pd.DataFrame(rows, columns=FEATURE_ORDER).fillna(0.0).values  # Matrice X
    prob = pipe.predict_proba(X)[:, 1]               # Proba de la classe ‚Äúrisque‚Äù
    out = pd.DataFrame(meta)                         # DF sorties
    out["prob_risk"] = prob                          # Ajoute proba
    out = out.sort_values("prob_risk", ascending=False)  # Trie d√©croissant

    print("\nTop (par proba pump-risk) :")
    for _, r in out.iterrows():                      # Affiche le ranking
        print(f" ‚Ä¢ {r['symbol']:<6s}  p={r['prob_risk']*100:5.1f}%")

    out.to_csv(out_csv, index=False)                 # Sauvegarde CSV
    print(f"\nüìÑ R√©sultats sauvegard√©s -> {out_csv}")
    if min_prob is not None:
        candidates = out[out["prob_risk"] >= float(min_prob)]  # Filtre sur seuil
        if not candidates.empty:
            print(f"\n‚ö†Ô∏è  Candidats ‚â• {min_prob:.2f} : " + ", ".join(candidates["symbol"].tolist()))  # Liste les alertes

# ==============================
# Menu sans sous-commande
# ==============================
def main_menu():
    choice = input("\nTon choix [1-4] (d√©faut 2): ").strip() or "2"  # Choix utilisateur
    if choice == "1":
        path = input(f"Chemin CSV √©v√©nements [{DEFAULT_EVENTS_CSV}]: ").strip() or DEFAULT_EVENTS_CSV  # Chemin CSV
        out  = input(f"Chemin mod√®le sortie [{DEFAULT_MODEL_PATH}]: ").strip() or DEFAULT_MODEL_PATH   # Chemin mod√®le out
        train_and_save(path, out)                     # Entra√Æne et sauvegarde
    elif choice == "2":
        model = input(f"Chemin du mod√®le [{DEFAULT_MODEL_PATH}]: ").strip() or DEFAULT_MODEL_PATH  # Chemin mod√®le
        if not os.path.exists(model):
            print("\nMod√®le introuvable ‚Äî entra√Ænement rapide avec CSV par d√©faut‚Ä¶")
            train_and_save(DEFAULT_EVENTS_CSV, model) # Entra√Æne si besoin
        syms = input(f"Symboles (s√©par√©s par des virgules) [vide = {DEFAULT_SYMBOLS_TXT} ou liste par d√©faut]: ").strip()  # Liste de tickers
        symbols = [s.strip().upper() for s in syms.split(",") if s.strip()] if syms else None  # Parse liste
        thr = input("Seuil min de proba (0..1) [0.5]: ").strip()  # Seuil d‚Äôalerte
        min_prob = float(thr) if ok(thr) else 0.5    # Conversion seuil
        predict_now(model, symbols, "pred_now.csv", min_prob)  # Lance la pr√©diction
    elif choice == "3":
        model = input(f"Chemin du mod√®le [{DEFAULT_MODEL_PATH}]: ").strip() or DEFAULT_MODEL_PATH  # Chemin mod√®le
        top = input("Top N features [15]: ").strip()  # Nombre de features √† afficher
        explain_model(model, int(top) if ok(top) else 15)  # Affiche l‚Äôimportance des features
    else:
        print("Bye.")                                 # Sortie

if __name__ == "__main__":
    try:
        main_menu()                                   # Lance le menu principal
    except KeyboardInterrupt:
        print("\nInterrompu.")                        # Gestion Ctrl+C propre
